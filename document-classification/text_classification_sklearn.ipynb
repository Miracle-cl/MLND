{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "from collections import Counter\n",
    "from string import punctuation, digits\n",
    "from time import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "%matplotlib inline\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义衡量标准\n",
    "def performance_metric(y_true, y_predict):\n",
    "    # 使用全局的预测正确率作为衡量标准\n",
    "    accuracy = f1_score( y_true, y_predict, average='micro' )\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词袋子模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 朴素贝叶斯-MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练最优模型 - 朴素贝叶斯\n",
    "def fit_NB(X, y, nplits=5):\n",
    "    \"\"\" 基于输入数据 [X,y]，利于网格搜索找到最优的朴素贝叶斯模型\"\"\"\n",
    "    \n",
    "    cross_validator = KFold( n_splits=nplits )\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "\n",
    "    params = { 'alpha': np.array([0.001, 0.01, 0.1, 1.0, 10]) }\n",
    "\n",
    "    scoring_fnc = make_scorer( performance_metric )\n",
    "\n",
    "    grid = GridSearchCV( clf, params, scoring=scoring_fnc, cv=cross_validator )\n",
    "\n",
    "    # 基于输入数据 [X,y]，进行网格搜索\n",
    "    grid = grid.fit(X, y)\n",
    "\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练集和测试集\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 20000) \n",
      "\n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True) \n",
      "\n",
      "0.794344131705\n"
     ]
    }
   ],
   "source": [
    "# 词向量获取方式： 词袋子模型 - 简单计数\n",
    "vectorizer_c = CountVectorizer(stop_words='english', max_features=20000)\n",
    "vectors_train = vectorizer_c.fit_transform(newsgroups_train.data)\n",
    "print(vectors_train.shape, '\\n')\n",
    "\n",
    "vectors_test = vectorizer_c.transform(newsgroups_test.data)\n",
    "\n",
    "optimal_clf = fit_NB(vectors_train, newsgroups_train.target)\n",
    "print(optimal_clf, '\\n')\n",
    "\n",
    "pred = optimal_clf.predict(vectors_test)\n",
    "\n",
    "accuracy_c = f1_score(newsgroups_test.target, pred, average='micro')\n",
    "print(accuracy_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 20000) \n",
      "\n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True) \n",
      "\n",
      "0.829394583112\n"
     ]
    }
   ],
   "source": [
    "# 词向量获取方式： 词袋子模型 - tf-idf\n",
    "vectorizer_ti = TfidfVectorizer(stop_words='english', max_features=20000)\n",
    "vectors_train = vectorizer_ti.fit_transform(newsgroups_train.data)\n",
    "print(vectors_train.shape, '\\n')\n",
    "\n",
    "vectors_test = vectorizer_ti.transform(newsgroups_test.data)\n",
    "\n",
    "optimal_clf = fit_NB(vectors_train, newsgroups_train.target)\n",
    "print(optimal_clf, '\\n')\n",
    "\n",
    "pred = optimal_clf.predict(vectors_test)\n",
    "\n",
    "accuracy_ti = f1_score(newsgroups_test.target, pred, average='micro')\n",
    "print(accuracy_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 20000) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.10000000000000001, class_prior=None, fit_prior=True) \n",
      "\n",
      "0.824880509825\n"
     ]
    }
   ],
   "source": [
    "# 词向量获取方式： 词袋子模型 - HashingVectorizer\n",
    "vectorizer_h = HashingVectorizer(stop_words='english', n_features=20000, non_negative=True)\n",
    "vectors_train = vectorizer_h.fit_transform(newsgroups_train.data)\n",
    "print(vectors_train.shape, '\\n')\n",
    "\n",
    "vectors_test = vectorizer_h.transform(newsgroups_test.data)\n",
    "\n",
    "optimal_clf = fit_NB(vectors_train, newsgroups_train.target)\n",
    "print(optimal_clf, '\\n')\n",
    "\n",
    "pred = optimal_clf.predict(vectors_test)\n",
    "\n",
    "accuracy_h = f1_score(newsgroups_test.target, pred, average='micro')\n",
    "print(accuracy_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 支持向量机-SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练最优模型 - linear SVM (SGDClassifier)\n",
    "def fit_svm(X, y, nplits=5):\n",
    "    \"\"\" 基于输入数据 [X,y]，利于网格搜索找到最优的支持向量机模型\"\"\"\n",
    "    \n",
    "    cross_validator = KFold( n_splits=nplits )\n",
    "    \n",
    "    clf = SGDClassifier(max_iter=5, tol=None)\n",
    "\n",
    "    params = {'alpha': (0.0001, 0.00001, 0.000001), \n",
    "              'penalty': ('l2', 'elasticnet')}\n",
    "\n",
    "    scoring_fnc = make_scorer( performance_metric )\n",
    "\n",
    "    grid = GridSearchCV( clf, params, scoring=scoring_fnc, cv=cross_validator )\n",
    "\n",
    "    # 基于输入数据 [X,y]，进行网格搜索\n",
    "    grid = grid.fit(X, y)\n",
    "\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 20000) \n",
      "\n",
      "Time: 22.00s\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False) \n",
      "\n",
      "0.842272968667\n"
     ]
    }
   ],
   "source": [
    "# 词向量获取方式： 词袋子模型 - tf-idf\n",
    "vectorizer_ti = TfidfVectorizer(stop_words='english', max_features=20000)\n",
    "vectors_train = vectorizer_ti.fit_transform(newsgroups_train.data)\n",
    "print(vectors_train.shape, '\\n')\n",
    "\n",
    "vectors_test = vectorizer_ti.transform(newsgroups_test.data)\n",
    "\n",
    "t0 = time()\n",
    "optimal_clf = fit_svm(vectors_train, newsgroups_train.target)\n",
    "print(\"Time: %.2fs\" % (time()-t0))\n",
    "print(optimal_clf, '\\n')\n",
    "\n",
    "pred = optimal_clf.predict(vectors_test)\n",
    "\n",
    "accuracy_ti = f1_score(newsgroups_test.target, pred, average='micro')\n",
    "print(accuracy_ti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 多层感知机-MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练最优模型 - 神经网络 MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# def fit_mlp(X, y, nplits=5):\n",
    "#     \"\"\"基于输入数据 [X,y]，利于随机搜索找到最优的多层感知机模型\"\"\"\n",
    "    \n",
    "#     cross_validator = KFold( n_splits=nplits )\n",
    "    \n",
    "#     clf = MLPClassifier()\n",
    "    \n",
    "#     # alpha: L2正则化\n",
    "#     params = {'alpha': (0.01, 0.001, 0.0001), \n",
    "#               'hidden_layer_sizes': ((128,), (256,), (512,))}\n",
    "\n",
    "#     scoring_fnc = make_scorer( performance_metric )\n",
    "\n",
    "#     grid = RandomizedSearchCV(clf, params, n_iter=6, scoring=scoring_fnc, cv=cross_validator, n_jobs=-1) # 6个随机参数集合\n",
    "\n",
    "#     # 基于输入数据 [X,y]，进行网格搜索\n",
    "#     grid = grid.fit(X, y)\n",
    "\n",
    "#     return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 20000) \n",
      "\n",
      "Time: 1039.52s\n",
      "0.836563993627\n"
     ]
    }
   ],
   "source": [
    "# 训练模型 - 神经网络 多层感知机\n",
    "# 词向量获取方式： 词袋子模型 - tf-idf\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "vectorizer_ti = TfidfVectorizer(stop_words='english', max_features=20000)\n",
    "vectors_train = vectorizer_ti.fit_transform(newsgroups_train.data)\n",
    "print(vectors_train.shape, '\\n')\n",
    "\n",
    "vectors_test = vectorizer_ti.transform(newsgroups_test.data)\n",
    "\n",
    "# alpha: L2正则化\n",
    "clf = MLPClassifier(hidden_layer_sizes=(512,), alpha=0.01)\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(vectors_train, newsgroups_train.target)\n",
    "print(\"Time: %.2fs\" % (time()-t0))\n",
    "\n",
    "pred = clf.predict(vectors_test)\n",
    "\n",
    "accuracy = f1_score(newsgroups_test.target, pred, average='micro')\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 梯度提升决策树-GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 训练最优模型 - GBDT - 随机搜索\n",
    "# def fit_GBDT(X, y, nplits=5):\n",
    "#     \"\"\" 基于输入数据 [X,y]，利于随机搜索找到最优的支持向量机模型\"\"\"\n",
    "    \n",
    "#     cross_validator = KFold( n_splits=nplits )\n",
    "    \n",
    "#     clf = GradientBoostingClassifier()\n",
    "\n",
    "#     params = {'n_estimators': (50, 100, 200, 300), \n",
    "#               'max_depth': (1, 3, 5, 7, 9)}\n",
    "\n",
    "#     scoring_fnc = make_scorer( performance_metric )\n",
    "\n",
    "#     grid = RandomizedSearchCV(clf, params, n_iter=6, scoring=scoring_fnc, cv=cross_validator, n_jobs=-1) # 6个随机参数集合\n",
    "\n",
    "#     # 基于输入数据 [X,y]，进行随机搜索\n",
    "#     grid = grid.fit(X, y)\n",
    "\n",
    "#     return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 20000) \n",
      "\n",
      "Time: 522.94s\n",
      "0.74614976102\n"
     ]
    }
   ],
   "source": [
    "# 训练模型 - GBDT\n",
    "# 词向量获取方式： 词袋子模型 - tf-idf\n",
    "vectorizer_ti = TfidfVectorizer(stop_words='english', max_features=20000)\n",
    "vectors_train = vectorizer_ti.fit_transform(newsgroups_train.data)\n",
    "print(vectors_train.shape, '\\n')\n",
    "\n",
    "vectors_test = vectorizer_ti.transform(newsgroups_test.data)\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(vectors_train, newsgroups_train.target)\n",
    "print(\"Time: %.2fs\" % (time()-t0))\n",
    "\n",
    "pred = clf.predict(vectors_test)\n",
    "\n",
    "accuracy = f1_score(newsgroups_test.target, pred, average='micro')\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 随机森林-Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 训练最优模型 - Random Forest\n",
    "# def fit_rf(X, y, nplits=5):\n",
    "#     \"\"\" 基于输入数据 [X,y]，利于随机搜索找到最优的支持向量机模型\"\"\"\n",
    "    \n",
    "#     cross_validator = KFold( n_splits=nplits )\n",
    "    \n",
    "#     clf = RandomForestClassifier()\n",
    "\n",
    "#     params = {'n_estimators': (5, 10, 15, 20), \n",
    "#               'max_depth': (None, 3, 5, 7, 9)}\n",
    "\n",
    "#     scoring_fnc = make_scorer( performance_metric )\n",
    "\n",
    "#     grid = RandomizedSearchCV(clf, params, n_iter=6, scoring=scoring_fnc, cv=cross_validator)\n",
    "\n",
    "#     # 基于输入数据 [X,y]，进行随机搜索\n",
    "#     grid = grid.fit(X, y)\n",
    "\n",
    "#     return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 20000) \n",
      "\n",
      "Time: 2.45s\n",
      "0.681492299522\n"
     ]
    }
   ],
   "source": [
    "# 训练模型 - Random Forest\n",
    "# 词向量获取方式： 词袋子模型 - tf-idf\n",
    "vectorizer_ti = TfidfVectorizer(stop_words='english', max_features=20000)\n",
    "vectors_train = vectorizer_ti.fit_transform(newsgroups_train.data)\n",
    "print(vectors_train.shape, '\\n')\n",
    "\n",
    "vectors_test = vectorizer_ti.transform(newsgroups_test.data)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(vectors_train, newsgroups_train.target)\n",
    "print(\"Time: %.2fs\" % (time()-t0))\n",
    "\n",
    "pred = clf.predict(vectors_test)\n",
    "\n",
    "accuracy = f1_score(newsgroups_test.target, pred, average='micro')\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主题模型-LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过主题模型获取主题向量来表示文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 7532\n"
     ]
    }
   ],
   "source": [
    "# 读取全集\n",
    "newsgroups_all = fetch_20newsgroups(subset='all').data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train').data\n",
    "newsgroups_test = fetch_20newsgroups(subset='test').data\n",
    "print(len(newsgroups_train), len(newsgroups_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pass'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从NLTK中导入WordNetLemmatizer 对词进行还原： is - be； dogs - dog\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "wordnet_lemmatizer.lemmatize('passed', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4990284\n"
     ]
    }
   ],
   "source": [
    "# 统计全集词列表\n",
    "words = []\n",
    "for text in newsgroups_all:\n",
    "    # 去除文档中的标点和数字，并全部转换成小写\n",
    "    text = ''.join([c for c in text.lower() if (c not in punctuation) and (c not in digits)])\n",
    "    # 将文档拆成单词，并进行词还原\n",
    "    words += [wordnet_lemmatizer.lemmatize(word, pos='v') for word in text.split()]\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 240506),\n",
       " ('be', 180873),\n",
       " ('to', 120304),\n",
       " ('of', 113840),\n",
       " ('a', 101126),\n",
       " ('and', 94986),\n",
       " ('in', 80337),\n",
       " ('i', 69387),\n",
       " ('that', 62564),\n",
       " ('have', 51823)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 组成全集集词汇表\n",
    "counts = Counter(words)\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从NLTK中引入停用词，从全集词汇表中删除\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "# 从词汇表中删除计数少于10次，或者字母长度大于20的单词 like 'maxaxaxaxaxaxaxaxaxaxaxaxaxaxax'\n",
    "vocab = {word: num for word, num in counts.items() if (word not in stoplist) and (num > 10) and (1 < len(word) < 20)}\n",
    "\n",
    "# 按照计数排序\n",
    "vocab_list = sorted(vocab, key=vocab.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17504\n"
     ]
    }
   ],
   "source": [
    "# 将单词映射到数字，从1开始\n",
    "vocab_to_int = {word: i for i ,word in enumerate(vocab_list, 1)}\n",
    "print(len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义训练集和测试集的预处理函数\n",
    "def pre_process_data(raw_data):\n",
    "    # 预处理数据特征\n",
    "    # raw data: 如 newsgroups_train.data = fetch_20newsgroups(subset='train').data \n",
    "    # seq_len: 序列长度\n",
    "    \n",
    "    # 将文档组成数字列表\n",
    "    all_texts = []\n",
    "    # length = []\n",
    "    for text in raw_data:\n",
    "        text = ''.join([c for c in text.lower() if c not in punctuation and c not in digits])\n",
    "        text = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in text.split()] # 分词和词还原\n",
    "        text = [word for word in text if word in vocab_to_int]\n",
    "        all_texts.append(text)\n",
    "        # length.append(len(text))  \n",
    "    return all_texts\n",
    "\n",
    "def pre_process_targets(raw_target, classes=20):\n",
    "    # 预处理数据标签 独热编码\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(list(range(classes)))\n",
    "    return lb.transform(raw_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mamatha', 'devineni', 'ratnam', 'mrandrewcmuedu', 'subject', 'pen', 'fan', 'reactions', 'organization', 'post', 'office', 'carnegie', 'mellon', 'pittsburgh', 'pa', 'line', 'nntppostinghost', 'poandrewcmuedu', 'sure', 'pen', 'fan', 'pretty', 'confuse', 'lack', 'kind', 'post', 'recent', 'pen', 'massacre', 'devil', 'actually', 'bite', 'puzzle', 'bite', 'relieve', 'however', 'go', 'put', 'end', 'relief', 'bite', 'praise', 'pen', 'man', 'kill', 'devil', 'worse', 'think', 'jagr', 'show', 'much', 'better', 'regular', 'season', 'stats', 'also', 'lot', 'fo', 'fun', 'watch', 'playoffs', 'bowman', 'let', 'jagr', 'lot', 'fun', 'next', 'couple', 'game', 'since', 'pen', 'go', 'beat', 'jersey', 'anyway', 'disappoint', 'see', 'islanders', 'lose', 'final', 'regular', 'season', 'game', 'pen', 'rule'], ['matthew', 'lawson', 'subject', 'highperformance', 'vlb', 'video', 'card', 'summary', 'seek', 'recommendations', 'vlb', 'video', 'card', 'nntppostinghost', 'organization', 'engineer', 'computer', 'network', 'university', 'oklahoma', 'norman', 'ok', 'usa', 'keywords', 'orchid', 'stealth', 'vlb', 'line', 'brother', 'market', 'highperformance', 'video', 'card', 'support', 'vesa', 'local', 'bus', 'mb', 'ram', 'anyone', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'ati', 'graphics', 'ultra', 'pro', 'highperformance', 'vlb', 'card', 'please', 'post', 'email', 'thank', 'matt', 'matthew', 'lawson', 'praise', 'exalt', 'glorify', 'king', 'heaven', 'everything', 'right', 'ways', 'king', 'babylon', 'bc']]\n"
     ]
    }
   ],
   "source": [
    "texts_all = pre_process_data(newsgroups_all)\n",
    "print(texts_all[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 按照texts_all中的单词创建词汇表\n",
    "dictionary = corpora.Dictionary(texts_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17504"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 6), (6, 2), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (27, 3), (28, 1), (29, 1), (30, 1), (31, 2), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 2), (45, 2), (46, 1), (47, 1), (48, 2), (49, 1), (50, 2), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 2), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1)]\n"
     ]
    }
   ],
   "source": [
    "# 按照dictionary创建每篇文档的词袋子模型，每篇文档由（单词对应的键，词频）组成\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_all]\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIme: 1076.5901086330414 s\n"
     ]
    }
   ],
   "source": [
    "# 生成 LDA 模型\n",
    "t0 = time()\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=100, id2word=dictionary, passes=20)\n",
    "print('TIme:', time()-t0, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存 LDA 模型\n",
    "ldamodel.save(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 0.034659902989598777),\n",
       " (29, 0.12140788217968064),\n",
       " (34, 0.012380261309099878),\n",
       " (37, 0.023731000663055946),\n",
       " (46, 0.032988322354522856),\n",
       " (56, 0.037178435603637684),\n",
       " (62, 0.014724935992155117),\n",
       " (83, 0.13375410752716363),\n",
       " (86, 0.30004839308932313),\n",
       " (93, 0.059263435018506672),\n",
       " (97, 0.2195144860639526)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.049311042569805758),\n",
       " (15, 0.12525604748038291),\n",
       " (16, 0.012551208473178406),\n",
       " (19, 0.22055373840238288),\n",
       " (48, 0.056972566440368216),\n",
       " (58, 0.088125372481612216),\n",
       " (68, 0.031962812940070189),\n",
       " (70, 0.050376290410712517),\n",
       " (87, 0.35225203191259746)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel[corpus[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14, 0.05099906556204923),\n",
       " (17, 0.013569792262001508),\n",
       " (37, 0.017703232392275985),\n",
       " (51, 0.014130242283191543),\n",
       " (59, 0.11075204678035798),\n",
       " (67, 0.052812930369317557),\n",
       " (68, 0.028664075030782572),\n",
       " (74, 0.3022055918047698),\n",
       " (82, 0.028401634730905217),\n",
       " (84, 0.072508114576306334),\n",
       " (89, 0.023222043769478905),\n",
       " (93, 0.02362039255569514),\n",
       " (97, 0.22766568597301823)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel[corpus[888]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_all = np.zeros((len(corpus), 100)) # 每一行代表一篇文本，每一列代表一个主题\n",
    "x_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    for tup in ldamodel[corpus[i]]:\n",
    "        idx = tup[0]\n",
    "        value = tup[1]\n",
    "        x_all[i, idx] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.03520696  0.          0.          0.\n",
      "  0.02663502  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.12098029  0.          0.          0.          0.\n",
      "  0.01238246  0.          0.          0.02365746  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.03222063\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.0371705   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.14060797  0.          0.          0.29660579  0.          0.\n",
      "  0.          0.          0.          0.          0.05966374  0.          0.\n",
      "  0.          0.20452036  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(x_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 100) (7532, 100)\n"
     ]
    }
   ],
   "source": [
    "# 和原始数据集大小保持基本一致\n",
    "x_train = x_all[:11314]\n",
    "x_test = x_all[11314:]\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  3, 17, ...,  3,  1,  7])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all_20 = fetch_20newsgroups(subset='all').target\n",
    "y_all_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314,) (7532,)\n"
     ]
    }
   ],
   "source": [
    "# 和原始数据集大小保持基本一致\n",
    "y_train_20 = y_all_20[:11314]\n",
    "y_test_20 = y_all_20[11314:]\n",
    "print(y_train_20.shape, y_test_20.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LDA - Naive Bayes， SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True) \n",
      "\n",
      "0.666356877323\n"
     ]
    }
   ],
   "source": [
    "# 朴素贝叶斯\n",
    "optimal_clf = fit_NB(x_train, y_train_20)\n",
    "print(optimal_clf, '\\n')\n",
    "\n",
    "pred = optimal_clf.predict(x_test)\n",
    "\n",
    "accuracy_c = f1_score(y_test_20, pred, average='micro')\n",
    "print(accuracy_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12.36s\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
      "       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False) \n",
      "\n",
      "0.646309081253\n"
     ]
    }
   ],
   "source": [
    "# 支持向量机\n",
    "\n",
    "t0 = time()\n",
    "optimal_clf = fit_svm(x_train, y_train_20)\n",
    "print(\"Time: %.2fs\" % (time()-t0))\n",
    "print(optimal_clf, '\\n')\n",
    "\n",
    "pred = optimal_clf.predict(x_test)\n",
    "\n",
    "accuracy_ti = f1_score(y_test_20, pred, average='micro')\n",
    "print(accuracy_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
